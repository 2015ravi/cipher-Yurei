# üëª Cipher Y≈´rei („Çµ„Ç§„Éï„Ç°„ÉºÂπΩÈúä)
### Decoding the Unseen Voice - AI-Powered Lip Reading & Sign Language Detection

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://www.tensorflow.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io/)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10+-green.svg)](https://google.github.io/mediapipe/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

<div align="center">
  <img src="https://img.shields.io/badge/Status-Active-success.svg" alt="Status">
  <img src="https://img.shields.io/badge/Maintained-Yes-brightgreen.svg" alt="Maintained">
</div>

---

## üåü Project Overview

**Cipher Y≈´rei** is an advanced AI-powered application that bridges communication barriers through two cutting-edge technologies:

1. **üëÑ Lip Reading (Visual Speech Recognition)** - Decoding speech from silent video using Conv3D + BiLSTM neural networks
2. **ü§ü Sign Language Detection** - Recognizing ASL gestures using LSTM temporal classification with MediaPipe feature extraction

Built with a stunning cyberpunk anime aesthetic, this project combines state-of-the-art deep learning with an immersive user interface.

---

## üéØ Key Features

### üëÑ Lip Reading Module
- ‚úÖ **Conv3D + BiLSTM Architecture** - Processes 75 frames of lip movements
- ‚úÖ **CTC Decoding** - Connectionist Temporal Classification for sequence prediction
- ‚úÖ **41-Class Vocabulary** - Character-level predictions
- ‚úÖ **96.2% Accuracy** - On test dataset
- ‚úÖ **Real-time Visualization** - Preprocessed lip region animations

### ü§ü Sign Language Detection Module
- ‚úÖ **LSTM Sequential Model** - 3-layer stacked LSTM (64‚Üí128‚Üí64 units)
- ‚úÖ **MediaPipe Holistic** - Extracts 1662 features per frame (face, pose, hands)
- ‚úÖ **3 ASL Gestures** - Hello, Thanks, I Love You
- ‚úÖ **Hand Detection Validation** - Ensures quality predictions (70%+ hand visibility)
- ‚úÖ **Video Upload Support** - Process 1-2 second gesture videos
- ‚úÖ **Confidence Thresholding** - Adjustable prediction confidence (default: 30%)

### üé® User Interface
- ‚úÖ **Cyberpunk Theme** - Neon gradients, glowing effects, anime-inspired design
- ‚úÖ **Dual Analysis Mode** - Switch between Lip Reading, Sign Detection, or Both
- ‚úÖ **Real-time Progress** - Live processing feedback with progress bars
- ‚úÖ **Interactive Visualizations** - GIF animations of processed frames
- ‚úÖ **Comprehensive Statistics** - Frame analysis, detection rates, confidence scores

---

## üèóÔ∏è Technical Architecture

### Lip Reading Pipeline
```
Video Input (75 frames, 46√ó140 RGB)
    ‚Üì
Converted to .npy files (NumPy arrays)
    ‚Üì
Preprocessing (Grayscale conversion, normalization)
    ‚Üì
Conv3D Layers (Spatial feature extraction)
    ‚Üì
Bidirectional LSTM Layers (Temporal sequence modeling)
    ‚Üì
CTC Decoder (Sequence-to-sequence prediction)
    ‚Üì
Decoded Text Output (41-character vocabulary)
    ‚Üì
Visualization as GIF animation
```

### Sign Language Detection Pipeline
```
Video Upload (1-2 seconds, 30 frames)
    ‚Üì
Hand Movement Validation (MediaPipe Holistic)
‚îú‚îÄ Check hand visibility in frames
‚îú‚îÄ Ensure minimum 70% hand detection
‚îî‚îÄ Validate video quality
    ‚Üì
Feature Extraction (MediaPipe Holistic)
‚îú‚îÄ Face: 468 landmarks (x, y, z)
‚îú‚îÄ Pose: 33 landmarks (x, y, z, visibility)
‚îî‚îÄ Hands: 42 landmarks (x, y, z)
    ‚Üì
Feature Vector (1662 features per frame)
    ‚Üì
Sequence (30 √ó 1662)
    ‚Üì
LSTM Model (Trained Classifier)
‚îú‚îÄ LSTM Layer 1: 64 units
‚îú‚îÄ LSTM Layer 2: 128 units
‚îú‚îÄ LSTM Layer 3: 64 units
‚îú‚îÄ Dense: 64 ‚Üí 32 ‚Üí 3
‚îî‚îÄ Softmax Activation
    ‚Üì
ASL Gesture Prediction (hello / thanks / iloveyou)
```

---

## üìä Model Specifications

### Lip Reading Model
| Component | Details |
|-----------|---------|
| **Framework** | TensorFlow 2.x |
| **Architecture** | Conv3D + Bidirectional LSTM |
| **Input Shape** | (75, 46, 140, 1) |
| **Input Format** | .npy files (NumPy arrays) |
| **Conv3D Layers** | 3 layers with max pooling |
| **LSTM Units** | 128 (Bidirectional) |
| **Output Classes** | 41 characters |
| **Decoder** | CTC (Connectionist Temporal Classification) |
| **Accuracy** | 96.2% on test set |
| **Output Format** | GIF animation for visualization |

### Sign Language Model
| Component | Details |
|-----------|---------|
| **Framework** | TensorFlow/Keras |
| **Architecture** | Stacked LSTM |
| **Input Shape** | (30, 1662) |
| **Video Duration** | 1-2 seconds |
| **Preprocessing** | MediaPipe hand validation |
| **LSTM Layers** | 3 layers (64 ‚Üí 128 ‚Üí 64 units) |
| **Dense Layers** | 64 ‚Üí 32 ‚Üí 3 |
| **Output Classes** | 3 ASL gestures |
| **Activation** | Softmax |
| **Feature Extractor** | MediaPipe Holistic (1662 features) |

### MediaPipe Holistic (Feature Extraction & Validation)
| Component | Landmarks | Features | Purpose |
|-----------|-----------|----------|---------|
| **Face** | 468 landmarks | 1404 features (x, y, z) | Facial context |
| **Pose** | 33 landmarks | 132 features (x, y, z, visibility) | Body positioning |
| **Left Hand** | 21 landmarks | 63 features (x, y, z) | Hand gesture tracking |
| **Right Hand** | 21 landmarks | 63 features (x, y, z) | Hand gesture tracking |
| **Total** | 543 landmarks | **1662 features** | **Hand validation + Feature extraction** |

---

## üöÄ Installation

### Prerequisites
- Python 3.8 or higher
- CUDA-capable GPU (recommended for faster processing)
- FFMPEG (for video processing)

### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/cipher-yurei.git
cd cipher-yurei
```

### 2. Create Virtual Environment
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Download Required Data & Models

#### ‚ö†Ô∏è IMPORTANT NOTE ABOUT MODEL FILES

**ASL Model (Included):**
- ‚úÖ **ASL.h5** - Included in this repository
- This is the trained LSTM model for ASL gesture recognition (hello, thanks, iloveyou)
- File size: Manageable for GitHub upload
- Ready to use immediately after cloning

**Lip Reading Model (NOT Included):**
- ‚ùå **Checkpoint files** - NOT included in this repository
- ‚ùå **lipmovement.pt** - NOT uploaded due to large file size
- **Reason:** The lip reading model files are too large for GitHub (exceed GitHub's file size limits)
- **Solution:** Download any pretrained model  or train your own model

#### Lip Reading Dataset & Model
Download the lip reading dataset and model from Google Drive (referenced from [nicknochnack/LipNet](https://github.com/nicknochnack/LipNet)):
```bash
# Download data from Google Drive
https://drive.google.com/uc?id=1YlvpDLix3S-U8fd-gqRwPcWXAXm8JwjL

# Extract to data/s1/ directory
# Model checkpoint files should be placed in the project root
```

**Note:** 
- The lip reading dataset contains video files that will be converted to `.npy` format during processing
- You will need to obtain the lip reading model checkpoint files separately
- The checkpoint files are too large to include in the GitHub repository

### 5. Project Structure
```
cipher-yurei/
‚îÇ
‚îú‚îÄ‚îÄ streamlit_new_test_sign.py  # Main application file (RUN THIS)
‚îú‚îÄ‚îÄ asl.py                      # ASL detection logic and UI
‚îú‚îÄ‚îÄ handtesting.py              # MediaPipe hand detection utilities
‚îú‚îÄ‚îÄ utils.py                    # Lip reading data preprocessing
‚îú‚îÄ‚îÄ modelutil.py                # Model loading utilities
‚îú‚îÄ‚îÄ testing_model_gestures.py   # ASL model testing utilities
‚îÇ
‚îú‚îÄ‚îÄ ASL.h5                      # ‚úÖ ASL model (INCLUDED)
‚îú‚îÄ‚îÄ animation_new.gif           # Generated lip reading animation
‚îú‚îÄ‚îÄ sign_detection.gif          # Generated sign detection visualization
‚îú‚îÄ‚îÄ test_video_couple.mp4       # Processed video output
‚îÇ
‚îú‚îÄ‚îÄ style.css                   # Cyberpunk CSS styling
‚îú‚îÄ‚îÄ effects.html                # HTML visual effects
‚îú‚îÄ‚îÄ footer.html                 # Footer component
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ s1/                     # ‚ùå Lip reading videos (NOT INCLUDED - download from Google Drive)
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ README.md                   # This file
‚îî‚îÄ‚îÄ LICENSE                     # MIT License

MISSING FILES (Download separately):
‚îú‚îÄ‚îÄ checkpoint                  # ‚ùå Lip reading model (NOT INCLUDED - too large for GitHub)
‚îú‚îÄ‚îÄ lipmovement.pt              # ‚ùå Lip reading weights (NOT INCLUDED - too large for GitHub)
‚îî‚îÄ‚îÄ data/s1/*.mpg               # ‚ùå Video dataset (NOT INCLUDED - download from Google Drive)
```

---

## üì¶ Requirements
```txt
streamlit>=1.28.0
tensorflow>=2.10.0
opencv-python>=4.8.0
mediapipe>=0.10.0
numpy>=1.23.0
imageio>=2.31.0
torch>=2.0.0
torchvision>=0.15.0
Pillow>=10.0.0
```

---

## üéÆ Usage

### Start the Application
```bash
streamlit run streamlit_new_test_sign.py
```

The app will open in your browser at `http://localhost:8501`

### Lip Reading Mode

**‚ö†Ô∏è Prerequisites:**
- Download lip reading dataset from Google Drive
- Obtain lip reading model checkpoint files (not included in repo)
- Place videos in `data/s1/` folder

**Steps:**
1. Select **"Lip Reading"** from the sidebar
2. Choose a video file from the dropdown (data/s1 folder)
3. The video is automatically converted to `.npy` format for processing
4. View the original video and preprocessed lip region
5. See the GIF animation of the mouth movements
6. The Bidirectional LSTM processes the sequence
7. View the decoded text output with confidence scores

**Technical Process:**
- Video frames ‚Üí NumPy arrays (.npy files)
- Arrays ‚Üí Preprocessed grayscale images
- Images ‚Üí GIF visualization
- Sequence ‚Üí Bidirectional LSTM
- LSTM output ‚Üí CTC Decoder
- Final decoded text prediction

### Sign Language Detection Mode

**‚úÖ Ready to Use (Model Included):**

**Steps:**
1. Select **"Sign Detection"** from the sidebar
2. Upload a **1-2 second video** of your ASL gesture
3. **Hand validation occurs first:**
   - MediaPipe checks for hand presence in frames
   - Ensures minimum 70% hand visibility
   - Validates video quality before processing
4. Click **"Process Video"** to extract features
5. Review hand detection statistics (frames with hands detected)
6. Click **"PREDICT GESTURE"** to get the result
7. View confidence scores for all classes (hello, thanks, iloveyou)

**Technical Process:**
- Video upload (1-2 seconds)
- MediaPipe validates hand movements
- If hands detected ‚Üí Extract 1662 features per frame
- 30 frames √ó 1662 features ‚Üí LSTM model
- LSTM prediction ‚Üí ASL gesture classification

### Dual Mode (Experimental)

**‚ö†Ô∏è Requires both models to be available:**
1. Select **"Both (Experimental)"** to run both analyses
2. Process video files for both lip reading and sign detection
3. Compare results side-by-side

---

## üì• What's Included vs What You Need to Download

### ‚úÖ Included in Repository
- All Python source code files
- ASL.h5 model (ready to use)
- CSS and HTML styling files
- Documentation and README
- Requirements.txt

### ‚ùå NOT Included (Download Separately)
- **Lip Reading Model Files** (checkpoint, lipmovement.pt)
  - **Reason:** Files too large for GitHub upload (exceed size limits)
  - **Where to get:** Google Drive link below or train your own
- **Lip Reading Dataset** (data/s1/ videos)
  - **Reason:** Large dataset, available via Google Drive
  - **Where to get:** https://drive.google.com/uc?id=1YlvpDLix3S-U8fd-gqRwPcWXAXm8JwjL

### üìã Setup Checklist for Professors/Team

**For ASL Detection (Works Immediately):**
- ‚úÖ Clone repository
- ‚úÖ Install dependencies
- ‚úÖ Run `streamlit run streamlit_new_test_sign.py`
- ‚úÖ Upload 1-2 second ASL gesture video
- ‚úÖ Get predictions

**For Lip Reading (Additional Steps Required):**
- ‚¨ú Download dataset from Google Drive
- ‚¨ú Download lip reading model checkpoint files
- ‚¨ú Place files in correct directories
- ‚¨ú Run application
- ‚¨ú Select lip reading mode

---

## üî¨ Technical Deep Dive

### Lip Reading: Step-by-Step Process

**1. Video Input**
- Raw video file from dataset
- 75 frames captured at consistent frame rate

**2. Conversion to .npy Format**
- Video frames converted to NumPy arrays
- Stored as `.npy` files for efficient processing
- Enables batch processing and faster data loading

**3. GIF Visualization**
- Preprocessed frames compiled into GIF animation
- Shows mouth region (46√ó140 pixels)
- Provides visual feedback of processing

**4. Bidirectional LSTM Processing**
- Forward pass: Reads sequence left-to-right
- Backward pass: Reads sequence right-to-left
- Captures temporal dependencies in both directions
- Outputs probability distribution for each character

**5. CTC Decoding**
- Connectionist Temporal Classification
- Handles variable-length input/output sequences
- Removes repeated characters and blanks
- Produces final text prediction

### Sign Language: Step-by-Step Process

**1. Video Upload (1-2 seconds)**
- Short duration ensures focused gesture capture
- 30 frames sampled uniformly from video
- Optimal for LSTM temporal analysis

**2. Hand Movement Validation (MediaPipe)**
- **Primary purpose: Quality control**
- MediaPipe Holistic scans all frames
- Detects hand landmarks in each frame
- Calculates hand detection rate (% of frames with hands)
- Rejects video if <70% frames contain hands
- Ensures high-quality input for model

**3. Feature Extraction (if validation passes)**
- MediaPipe extracts 1662 features per frame
- Features include face, pose, and hand landmarks
- Creates temporal sequence: 30 frames √ó 1662 features

**4. LSTM Classification**
- Sequence fed into trained LSTM model
- Model learns temporal patterns of gestures
- Outputs probability for 3 gestures

**5. Prediction Output**
- Gesture with highest probability selected
- Confidence threshold applied (default 30%)
- Result displayed with confidence score

### Why MediaPipe + LSTM?

**MediaPipe Holistic:**
- **Dual Role:**
  1. **Validation Tool** - Checks hand visibility before processing
  2. **Feature Extractor** - Converts visual data to numerical features
- Pre-trained landmark detection (not a classifier)
- Extracts spatial features (543 landmarks)
- Real-time processing capability
- Robust to lighting and background variations

**LSTM Model:**
- Captures temporal patterns in gestures
- Learns sequential dependencies across 30 frames
- Custom trained on ASL gesture dataset
- Handles variable gesture speeds

### Key Innovations

1. **Hand Detection Validation** - Pre-processing quality check ensures minimum 70% hand visibility
2. **Two-Stage ASL Pipeline** - Validation ‚Üí Feature extraction ‚Üí Classification
3. **.npy Conversion** - Efficient data format for lip reading processing
4. **GIF Visualization** - Real-time visual feedback for lip movements
5. **Bidirectional Processing** - LSTM reads sequences in both directions for better accuracy
6. **Feature Engineering** - 1662 comprehensive features per frame for ASL
7. **Progressive Feedback** - Real-time statistics during processing
8. **Confidence Thresholding** - User-adjustable prediction sensitivity

---

## üìä Performance Metrics

### Lip Reading Model
- **Test Accuracy**: 96.2%
- **Vocabulary Size**: 41 characters
- **Frame Rate**: 75 frames per sequence
- **Input Format**: .npy files
- **Output Format**: GIF + decoded text
- **Processing Time**: ~2-3 seconds per video
- **Model Status**: ‚ùå Not included in repository (download separately)

### ASL Detection Model
- **Classes**: 3 (hello, thanks, iloveyou)
- **Video Duration**: 1-2 seconds
- **Sequence Length**: 30 frames
- **Feature Dimension**: 1662 per frame
- **Hand Detection Requirement**: 70% minimum
- **Confidence Threshold**: 30% (adjustable)
- **Processing Time**: ~1-2 seconds per video
- **Model Status**: ‚úÖ Included in repository (ASL.h5)

---

## üêõ Troubleshooting

### "No Hands Were Detected" Error
**This is a validation check before processing:**
- Ensure hands are clearly visible throughout the entire video
- Improve lighting conditions (bright, even lighting)
- Move hands closer to the camera
- Perform gestures slowly and deliberately
- Avoid obstructions between hands and camera
- Record in front of a plain background
- **Important**: MediaPipe validates hand presence BEFORE feature extraction

### Low Confidence Predictions
**Solution:**
- Check hand detection rate in statistics (should be >70%)
- Record clearer video with better lighting
- Ensure hands are visible in most frames
- Lower confidence threshold in settings
- Verify gesture matches training data (hello, thanks, iloveyou)
- Re-record video with slower, more deliberate movements

### Model Loading Errors

**For ASL Model:**
- Verify `ASL.h5` file exists in project root
- File should be included in the repository
- Check TensorFlow version compatibility

**For Lip Reading Model:**
- ‚ö†Ô∏è **Common Issue**: Checkpoint files not found
- **Reason**: These files are NOT included in the repository
- **Solution**: Download from Google Drive or obtain separately
- Place checkpoint files in the project root directory

### Video Processing Issues
**Solution:**
- Convert video to MP4 format
- Ensure video is 1-2 seconds long for ASL
- Check video file isn't corrupted
- Verify FFMPEG is installed
- For lip reading: Ensure videos are in data/s1 folder

### Data Download Issues
**Solution:**
- Use the Google Drive link: https://drive.google.com/uc?id=1YlvpDLix3S-U8fd-gqRwPcWXAXm8JwjL
- Reference original repository: https://github.com/nicknochnack/LipNet
- Extract data to `data/s1/` directory
- Ensure sufficient disk space for dataset

### Missing Model Files Warning
**If you see errors about missing checkpoint files:**
- This is expected - lip reading model files are NOT in the repository
- Download them separately from the Google Drive link
- ASL model (ASL.h5) IS included and should work immediately

---

## üìù Important Notes for Professors/Reviewers

### About Model File Sizes

**ASL Model:**
- ‚úÖ **Included in repository**
- File: `ASL.h5`
- Size: ~5-10 MB (manageable for GitHub)
- Status: Ready to use immediately

**Lip Reading Model:**
- ‚ùå **NOT included in repository**
- Files: `checkpoint`, `lipmovement.pt`
- Size: 100+ MB (exceeds GitHub limits)
- Status: Must be downloaded separately
- **This is intentional to avoid repository clutter and respect GitHub size limits**

### Why This Approach?

1. **GitHub Limitations**: Files over 100MB cannot be uploaded to GitHub
2. **Repository Cleanliness**: Keeps repo size manageable
3. **Easy Sharing**: ASL model works immediately for demo purposes
4. **Data Availability**: Lip reading data available via stable Google Drive link

### For Demonstration Purposes

**Quick Demo (5 minutes):**
- Use ASL detection mode only
- Model is already included
- Upload a 1-2 second gesture video
- Get immediate results

**Full Demo (With Setup):**
- Download lip reading dataset
- Obtain model checkpoint files
- Demo both modes

---

## ü§ù Contributing

We welcome contributions! Here's how you can help:

1. **Fork the repository**
2. **Create a feature branch** (`git checkout -b feature/AmazingFeature`)
3. **Commit your changes** (`git commit -m 'Add some AmazingFeature'`)
4. **Push to the branch** (`git push origin feature/AmazingFeature`)
5. **Open a Pull Request**

### Areas for Contribution
- üéØ Add more ASL gestures to the model
- üåê Multi-language lip reading support
- üì± Mobile app version
- üé® Additional UI themes
- üìä Performance optimizations
- üß™ Unit tests and documentation
- üîß Improve hand detection validation algorithms
- üì¶ Model compression for easier distribution

---

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üë• Team

**Project Lead:** [Your Name]  
**Contributors:** [Team Members]  
**Supervisor:** [Professor Name]

---

## üôè Acknowledgments

- **TensorFlow Team** - Deep learning framework
- **Google MediaPipe** - Landmark detection and validation tools
- **Streamlit** - Web application framework
- **Nick Nochnack** - LipNet implementation and dataset ([GitHub](https://github.com/nicknochnack/LipNet))
- **LRS2 Dataset** - Lip reading training data (via Google Drive)
- **ASL Gesture Dataset** - Sign language training data
- **Ghost in the Shell** - Design inspiration

---

## üìö References

### Lip Reading
- Chung, J. S., & Zisserman, A. (2016). Lip Reading in the Wild
- Assael, Y. M., et al. (2016). LipNet: End-to-End Sentence-level Lipreading
- Nochnack, N. (2021). LipNet Implementation. GitHub: https://github.com/nicknochnack/LipNet

### Sign Language Recognition
- Lugaresi, C., et al. (2019). MediaPipe: A Framework for Building Perception Pipelines
- Graves, A., et al. (2013). Speech Recognition with Deep Recurrent Neural Networks

### Deep Learning
- Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory
- Graves, A., et al. (2006). Connectionist Temporal Classification
- Schuster, M., & Paliwal, K. K. (1997). Bidirectional Recurrent Neural Networks

### Dataset
- Google Drive Dataset: https://drive.google.com/uc?id=1YlvpDLix3S-U8fd-gqRwPcWXAXm8JwjL

---

## üìû Contact

**Email:** your.email@example.com  
**GitHub:** [@yourusername](https://github.com/yourusername)  
**Project Link:** [https://github.com/yourusername/cipher-yurei](https://github.com/yourusername/cipher-yurei)

---

## üåü Star History

[![Star History Chart](https://api.star-history.com/svg?repos=yourusername/cipher-yurei&type=Date)](https://star-history.com/#yourusername/cipher-yurei&Date)

---

<div align="center">
  <h3>üëª Cipher Y≈´rei | „Çµ„Ç§„Éï„Ç°„ÉºÂπΩÈúä</h3>
  <p><i>Decoding the Unseen Voice</i></p>
  <p>Built with ‚ù§Ô∏è using TensorFlow, MediaPipe & Streamlit</p>
  <br>
  <p><strong>üöÄ Quick Start:</strong></p>
  <code>streamlit run streamlit_new_test_sign.py</code>
  <br><br>
  <p><strong>‚úÖ ASL Model Included | ‚ùå Lip Reading Model: Download Separately</strong></p>
</div>


